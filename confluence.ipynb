{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 274/274 [00:03<00:00, 83.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import openai\n",
    "import pinecone\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "from llama_hub.confluence.base import ConfluenceReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import LLMPredictor, ServiceContext, GPTVectorStoreIndex\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "openai.organization = config[\"ORG_ID\"]\n",
    "openai.api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# os.environ[\"CONFLUENCE_API_TOKEN\"] = config[\"CONFLUENCE_API_TOKEN\"]\n",
    "# os.environ[\"CONFLUENCE_USERNAME\"] = config[\"CONFLUENCE_USERNAME\"]\n",
    "\n",
    "# set the size of the context window of the LLM. Typically this is set automatically with the model metadata. We can also explicitly override via this parameter for additional control\n",
    "context_window = 4096\n",
    "# set number of output tokens from the LLM. Typically this is set automatically with the model metadata. It doesn't limit the model output, but affects the amount of “space” we save for the output, when computing available context window size for packing text from retrieved Nodes\n",
    "num_output = 512\n",
    "\n",
    "#LLMPredictor is a wrapper class around LangChain's LLMChain that allows easy integration into LlamaIndex\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\", max_tokens=num_output))\n",
    "\n",
    "#constructs service_context\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, \n",
    "                                                context_window=context_window,\n",
    "                                                num_output=num_output)\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "# init pinecone\n",
    "pinecone.init(api_key=config['PINECONE_API_KEY'], environment=\"asia-southeast1-gcp-free\")\n",
    "\n",
    "def data_ingestion_indexing():\n",
    "\n",
    "    # Confluence base url and space key\n",
    "    base_url = \"https://navihq.atlassian.net/wiki/\"\n",
    "    space_key = \"IN\"\n",
    "    \n",
    "\n",
    "    # construct ConfluenceReader and load data from Confluence\n",
    "    loader = ConfluenceReader(base_url=base_url)\n",
    "    documents = loader.load_data(space_key=space_key, page_ids=[], include_attachments=False)\n",
    "    # set pinecone index\n",
    "    pinecone_index = pinecone.Index(\"confluence-index\")\n",
    "    # build the PineconeVectorStore and GPTVectorStoreIndex\n",
    "    vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "    return index\n",
    "\n",
    "index = data_ingestion_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may need to refresh the ConsoleMe portal a few times to see the roles that have been assigned to you. If the issue persists, you should reach out to the Infra/Security team for further assistance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def data_querying(input_text):\n",
    "    # res = openai.Embedding.create(\n",
    "    #     input=[\n",
    "    #         input_text\n",
    "    #     ], engine=\"text-embedding-ada-002\"\n",
    "    # )\n",
    "    # embeds = [record['embedding'] for record in res['data']]\n",
    "    # result = index.as_query_engine(service_context=service_context).query(embeds[0])\n",
    "    result = index.as_query_engine(service_context=service_context).query(input_text)\n",
    "    return result\n",
    "\n",
    "print(data_querying(\"I’m not able to see any role on consoleMe, I’ve checked with security team, roles have been assigned to me, but not visible on the portal\\\n",
    "This is urgent , please check\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
