{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x2803f04f0 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:150\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m list_of_text \u001b[39m=\u001b[39m [text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m list_of_text]\n\u001b[0;32m--> 150\u001b[0m data \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mlist_of_text, model\u001b[39m=\u001b[39;49mengine, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mdata\n\u001b[1;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m [d[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/openai/api_resources/embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m     \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39m# This is only for the default case.\u001b[39;00m\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    138\u001b[0m (\n\u001b[1;32m    139\u001b[0m     deployment_id,\n\u001b[1;32m    140\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m     url,\n\u001b[1;32m    156\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     index \u001b[39m=\u001b[39m GPTVectorStoreIndex\u001b[39m.\u001b[39mfrom_documents(documents, storage_context\u001b[39m=\u001b[39mstorage_context)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m index\n\u001b[0;32m---> 54\u001b[0m index \u001b[39m=\u001b[39m data_ingestion_indexing()\n",
      "Cell \u001b[0;32mIn[49], line 51\u001b[0m, in \u001b[0;36mdata_ingestion_indexing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m vector_store \u001b[39m=\u001b[39m PineconeVectorStore(pinecone_index\u001b[39m=\u001b[39mpinecone_index)\n\u001b[1;32m     50\u001b[0m storage_context \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults(vector_store\u001b[39m=\u001b[39mvector_store)\n\u001b[0;32m---> 51\u001b[0m index \u001b[39m=\u001b[39m GPTVectorStoreIndex\u001b[39m.\u001b[39;49mfrom_documents(documents, storage_context\u001b[39m=\u001b[39;49mstorage_context)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m index\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/base.py:97\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39mget_doc_id(), doc\u001b[39m.\u001b[39mget_doc_hash())\n\u001b[1;32m     95\u001b[0m nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39mnode_parser\u001b[39m.\u001b[39mget_nodes_from_documents(documents)\n\u001b[0;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m     98\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     99\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m    100\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m    101\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    102\u001b[0m )\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:45\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_nodes_override \u001b[39m=\u001b[39m store_nodes_override\n\u001b[0;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     46\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     47\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     48\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     49\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m     50\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/base.py:68\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[39massert\u001b[39;00m nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     index_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_index_from_nodes(nodes)\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct \u001b[39m=\u001b[39m index_struct\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39madd_index_struct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[39mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:221\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39m@llm_token_counter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbuild_index_from_nodes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_index_from_nodes\u001b[39m(\u001b[39mself\u001b[39m, nodes: Sequence[Node]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m IndexDict:\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build the index from nodes.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m \u001b[39m    NOTE: Overrides BaseIndex.build_index_from_nodes.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m        VectorStoreIndex only stores nodes in document store\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m        if vector store does not store text\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index_from_nodes(nodes)\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:210\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    208\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    209\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_nodes_to_index(index_struct, nodes)\n\u001b[1;32m    211\u001b[0m \u001b[39mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:186\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nodes:\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m embedding_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_node_embedding_results(nodes)\n\u001b[1;32m    187\u001b[0m new_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39madd(embedding_results)\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mstores_text \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    190\u001b[0m     \u001b[39m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[39m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:105\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_embedding_results\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m     99\u001b[0m         id_to_embed_map[n\u001b[39m.\u001b[39mget_doc_id()] \u001b[39m=\u001b[39m n\u001b[39m.\u001b[39membedding\n\u001b[1;32m    101\u001b[0m \u001b[39m# call embedding model to get embeddings\u001b[39;00m\n\u001b[1;32m    102\u001b[0m (\n\u001b[1;32m    103\u001b[0m     result_ids,\n\u001b[1;32m    104\u001b[0m     result_embeddings,\n\u001b[0;32m--> 105\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_service_context\u001b[39m.\u001b[39;49membed_model\u001b[39m.\u001b[39;49mget_queued_text_embeddings()\n\u001b[1;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m new_id, text_embedding \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(result_ids, result_embeddings):\n\u001b[1;32m    107\u001b[0m     id_to_embed_map[new_id] \u001b[39m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/embeddings/base.py:168\u001b[0m, in \u001b[0;36mBaseEmbedding.get_queued_text_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m cur_batch_ids \u001b[39m=\u001b[39m [text_id \u001b[39mfor\u001b[39;00m text_id, _ \u001b[39min\u001b[39;00m cur_batch]\n\u001b[1;32m    167\u001b[0m cur_batch_texts \u001b[39m=\u001b[39m [text \u001b[39mfor\u001b[39;00m _, text \u001b[39min\u001b[39;00m cur_batch]\n\u001b[0;32m--> 168\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_text_embeddings(cur_batch_texts)\n\u001b[1;32m    169\u001b[0m result_ids\u001b[39m.\u001b[39mextend(cur_batch_ids)\n\u001b[1;32m    170\u001b[0m result_embeddings\u001b[39m.\u001b[39mextend(embeddings)\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:267\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_text_embeddings\u001b[39m(\u001b[39mself\u001b[39m, texts: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[List[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m    261\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[39m    By default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    Can be overriden for batch queries.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \n\u001b[1;32m    266\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m get_embeddings(\n\u001b[1;32m    268\u001b[0m         texts,\n\u001b[1;32m    269\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_engine,\n\u001b[1;32m    270\u001b[0m         deployment_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment_name,\n\u001b[1;32m    271\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopenai_kwargs,\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/pincode/navi-hackathon/.venv/lib/python3.9/site-packages/tenacity/__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[1;32m    325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39mreraise()\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n\u001b[1;32m    329\u001b[0m     sleep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait(retry_state)\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x2803f04f0 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import openai\n",
    "import pinecone\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "from llama_hub.confluence.base import ConfluenceReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import LLMPredictor, ServiceContext, GPTVectorStoreIndex\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "openai.organization = config[\"ORG_ID\"]\n",
    "openai.api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# set the size of the context window of the LLM. Typically this is set automatically with the model metadata. We can also explicitly override via this parameter for additional control\n",
    "context_window = 4096\n",
    "# set number of output tokens from the LLM. Typically this is set automatically with the model metadata. It doesn't limit the model output, but affects the amount of “space” we save for the output, when computing available context window size for packing text from retrieved Nodes\n",
    "num_output = 512\n",
    "\n",
    "#LLMPredictor is a wrapper class around LangChain's LLMChain that allows easy integration into LlamaIndex\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\", max_tokens=num_output))\n",
    "\n",
    "#constructs service_context\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, \n",
    "                                                context_window=context_window,\n",
    "                                                num_output=num_output)\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "# init pinecone\n",
    "pinecone.init(api_key=config['PINECONE_API_KEY'], environment=\"asia-southeast1-gcp-free\")\n",
    "\n",
    "def data_ingestion_indexing():\n",
    "\n",
    "    # Confluence base url and space key\n",
    "    base_url = \"https://navihq.atlassian.net/wiki/\"\n",
    "    space_key = \"IN\"\n",
    "    \n",
    "\n",
    "    # construct ConfluenceReader and load data from Confluence\n",
    "    loader = ConfluenceReader(base_url=base_url)\n",
    "    documents = loader.load_data(space_key=space_key, page_ids=[], include_attachments=False)\n",
    "    # set pinecone index\n",
    "    pinecone_index = pinecone.Index(\"confluence-index\")\n",
    "    # build the PineconeVectorStore and GPTVectorStoreIndex\n",
    "    vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "    return index\n",
    "\n",
    "index = data_ingestion_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is possible that the log ingestion limit has been reduced if the service logs are being throttled despite emitting a similar log count as before. This is because the threshold for throttling is based on the log ingestion rate, which is determined by the Prometheus query mentioned in the context information. If the ingestion rate exceeds the threshold, logs will be dropped and the threshold may be reduced over time if the service continues to exceed it. It is important for teams to take appropriate action to control log ingestion when they receive alerts about throttling. \n",
      "\n",
      "Sources: \n",
      "- Context information: \"Any log ingested over the throttling threshold will be dropped and not be available at any point in time. Teams already receive alerts when their service 's logs are throttled and appropriate action is required to control the ingestion.\"\n",
      "- Context information: \"If the ingestion rate fails to come down below the original throttling limit, the threshold will keep getting reduced every 12 hours.\"\n"
     ]
    }
   ],
   "source": [
    "query_suffix = \".Use bullet points wherever applicable. Please always cite sources along with your answer.\"\n",
    "\n",
    "def data_querying(input_text):\n",
    "    result = index.as_query_engine(service_context=service_context).query(input_text + query_suffix)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(data_querying(\"service logs are being throtled. However we are emitting similar log count as before. Is the log ingestion limit reduced?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
